{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Filename: Binary_Relevance_Classification.ipynb\n",
    "#\n",
    "# Purpose: Multi-label Text-categorization via binary relevance, using k-nearest neighbors, gradient boosted trees, \n",
    "#          and support vector machines.\n",
    "\n",
    "# Author(s): Bobby (Robert) Lumpkin, Archit Datar (kNN)\n",
    "#\n",
    "# Library Dependencies: numpy, pandas, scikit-learn\n",
    "########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import scipy\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.neighbors import KNeighborsClassifier as kNN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from joblib import dump, load\n",
    "import sys\n",
    "sys.path.append('../../../ThresholdFunctionLearning')    ## Append path to the ThresholdFunctionLearning directory to the interpreters\n",
    "                                                   ## search path\n",
    "from threshold_learning import predict_test_labels_binary    ## Import the 'predict_test_labels_binary()' function \n",
    "import os\n",
    "os.chdir('C:\\\\Users\\\\rober\\\\OneDrive\\\\Documents\\\\Multilabel-Text-Classification\\\\Binary Relevance Models\\\\kNN Based')  \n",
    "## Replace with above path with appropriate working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel Classification Using Binary Relevance Models\n",
    "\n",
    "Arguably, the most intuitive among multilabel modeling approaches is what's referred to as \"binary relevance\". This approach works by decomposing the multi-label learning task into a number of independent binary learning tasks (one per class label) (Zhang et al. [2018]). Binary Relevance methods are often criticized in the literature because of their label independence assumption, producing a potential weakness of ignoring correlations among labels (Luaces et al. [2012]). In this notebook, we'll explore binary relevance models built using differenct base classifiers. Later, in other notebooks, we'll train more novel approaches for comparison.\n",
    "\n",
    "Each of our models will be evaluated using the default constant threshold function of $t(x) \\equiv 0.5$ in addition to using a learned threshold function. Learning threshold functions has the advantage of allowing for different instances to possess different thresholds. Thsi can be useful, when a model either cannot consistenty separate true from false labels around a constant value OR when sufficient training is resource intensive. In many instances, a model may learn to separate true from false labels earlier in the training process than it learns to separate about a constant threshold.\n",
    "\n",
    "Additionally, each of the models will be trained using both the separable principal component scores and the autoencoder encodings derived in 'Preprocessing and Dimension Reduction/tfidf_Dimension_Reduction.ipynb'. Below, we'll load the data and compute one baseline for validating our models according to Hamming Loss. Namely, since our labels are sparse, we'll compute the validation Hamming Loss associated with a constant zero classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the 'separable' PC features\n",
    "npzfile = np.load(\"../../Data/tfidf_PC_separable.npz\")\n",
    "X_sepPCs_train = npzfile[\"X_sepPCs_train\"]\n",
    "X_sepPCs_test = npzfile[\"X_sepPCs_test\"]\n",
    "\n",
    "## Load the autoencoder encodings\n",
    "npzfile = np.load(\"../../Data/tfidf_encoded_data.npz\")\n",
    "encoded_train = npzfile[\"encoded_train\"]\n",
    "encoded_test = npzfile[\"encoded_test\"]\n",
    "\n",
    "## Load the labels\n",
    "Y_train = npzfile[\"Y_train\"]\n",
    "Y_test = npzfile[\"Y_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013779397151374627"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Compute the validation Hamming Loss for a constant zero classifier (used as silly baseline for sparse labels)\n",
    "prop_one_bpmll = np.sum(Y_test == 1) / (Y_test.shape[0] * Y_test.shape[1])\n",
    "prop_one_bpmll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Classifier: k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Hamming loss for the training data is 0.003\n",
      "The Hamming loss for the test data is 0.005\n"
     ]
    }
   ],
   "source": [
    "## Implement a binary relevance model using KNN classifiers (Naive approach to be compared with ML-KNN, later)\n",
    "br_classifier = BinaryRelevance(\n",
    "    classifier = kNN()\n",
    ")\n",
    "\n",
    "br_classifier.fit(X_sepPCs_train, Y_train)\n",
    "\n",
    "#br_train_preds = br_classifier.predict(X_sepPCs_train).toarray() -- Making predictions takes some time. \n",
    "#br_test_preds = br_classifier.predict(X_sepPCs_test).toarray()      Instead, load the predictions from 'kNN_based_preds.npz', on next line.\n",
    "\n",
    "npzfile = npzfile = np.load(\"kNN_based_preds.npz\", allow_pickle = True)\n",
    "br_train_preds = npzfile[\"br_train_preds\"]\n",
    "br_test_preds = npzfile[\"br_test_preds\"]\n",
    "\n",
    "print (f\"The Hamming loss for the training data is {metrics.hamming_loss(Y_train, br_train_preds):.3f}\")\n",
    "print (f\"The Hamming loss for the test data is {metrics.hamming_loss(Y_test, br_test_preds):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (CAUTION: DO NOT OVERWRITE EXISTING FILES) -- Save the kNN based predictions\n",
    "outfile = \"kNN_based_preds\"\n",
    "#np.savez_compressed(outfile, br_train_preds = br_train_preds,\n",
    "#                             br_test_preds = br_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "random.seed(123)\n",
    "n_neighbors_list = list(range(3, 16, 3))\n",
    "n_neighbors_list.insert(0, 1)\n",
    "parameters_br = {'n_neighbors': n_neighbors_list}  \n",
    "# By default, the Hamming loss as an option is not provided in the scoring string options. \n",
    "# So, we first define a Hamming loss scorer and use that. \n",
    "hamming_scorer = metrics.make_scorer(metrics.hamming_loss)\n",
    "\n",
    "clf_br = GridSearchCV(kNN(), parameters_br, scoring = hamming_scorer, cv = 5, verbose = 1)\n",
    "#clf_br.fit(X_sepPCs_train, Y_train) #-- To save time, load the pre-fit grid search object in the next line.\n",
    "clf_br = load(\"clf_br_gridSearch_object.joblib\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kNN Based/clf_br_gridSearch_object.joblib']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## (CAUTION: DO NOT OVERWRITE EXISTING FILES) -- Save the kNN based predictions\n",
    "outfile = \"clf_br_gridSearch_object.joblib\"\n",
    "#dump(clf_br, outfile, compress = 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Params</th>\n",
       "      <th>Mean out-of-bag Hamming loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'n_neighbors': 1}</td>\n",
       "      <td>0.005997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'n_neighbors': 3}</td>\n",
       "      <td>0.004788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'n_neighbors': 6}</td>\n",
       "      <td>0.004697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'n_neighbors': 9}</td>\n",
       "      <td>0.004714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'n_neighbors': 12}</td>\n",
       "      <td>0.004881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'n_neighbors': 15}</td>\n",
       "      <td>0.004877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Params  Mean out-of-bag Hamming loss\n",
       "0   {'n_neighbors': 1}                      0.005997\n",
       "1   {'n_neighbors': 3}                      0.004788\n",
       "2   {'n_neighbors': 6}                      0.004697\n",
       "3   {'n_neighbors': 9}                      0.004714\n",
       "4  {'n_neighbors': 12}                      0.004881\n",
       "5  {'n_neighbors': 15}                      0.004877"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_neighbors': 6}. Best mean out-of-bag Hamming loss: 0.004696695591737446\n"
     ]
    }
   ],
   "source": [
    "best_index_br = np.argmin(clf_br.cv_results_[\"mean_test_score\"])\n",
    "best_parameters_br = clf_br.cv_results_[\"params\"][best_index_br]\n",
    "\n",
    "df_CV_br = pd.DataFrame(columns=[\"Params\", \"Mean out-of-bag Hamming loss\"])\n",
    "df_CV_br[\"Params\"] = clf_br.cv_results_[\"params\"]\n",
    "df_CV_br[ \"Mean out-of-bag Hamming loss\"] = clf_br.cv_results_[\"mean_test_score\"]\n",
    "display(df_CV_br)\n",
    "print(f\"Best parameters: {best_parameters_br}. Best mean out-of-bag Hamming loss: {np.min(clf_br.cv_results_['mean_test_score'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: The Hamming loss training data is 0.004\n",
      "Best parameters: The Hamming loss test data is 0.005\n",
      "Best parameters with threshold function learning: Hamming loss Test set is 0.009355562916344632\n"
     ]
    }
   ],
   "source": [
    "# Threshold learning using the best parameters from the cross-validated grid search \n",
    "classifier_best_br = BinaryRelevance(\n",
    "    classifier = kNN(n_neighbors = best_parameters_br['n_neighbors'])\n",
    ")\n",
    "\n",
    "classifier_best_br.fit(X_sepPCs_train, Y_train)\n",
    "#Y_train_pred_best_br = classifier_best_br.predict(X_sepPCs_train) # -- These 'predict()' steps can be time costly.\n",
    "#Y_train_pred_best_array_br = Y_train_pred_best_br.toarray()          # Instead, load the predictions below\n",
    "#Y_test_pred_best_br = classifier_best_br.predict(X_sepPCs_test)\n",
    "#Y_test_pred_best_array_br = Y_test_pred_best_br.toarray()\n",
    "\n",
    "npzfile = np.load(\"kNN_bestModel_preds.npz\", allow_pickle = True)\n",
    "Y_train_pred_best_array_br = npzfile[\"Y_train_pred_best_array_br\"]\n",
    "Y_test_pred_best_array_br = npzfile[\"Y_test_pred_best_array_br\"]\n",
    "Y_train_pred_proba_array = npzfile[\"Y_train_pred_proba_array\"]\n",
    "Y_test_pred_proba_array = npzfile[\"Y_test_pred_proba_array\"]\n",
    "\n",
    "threshold_function = load(\"learned_threshold_function.joblib\")\n",
    "\n",
    "print (f\"Best parameters: The Hamming loss training data is {metrics.hamming_loss(Y_train, Y_train_pred_best_array_br):.3f}\")\n",
    "print (f\"Best parameters: The Hamming loss test data is {metrics.hamming_loss(Y_test, Y_test_pred_best_array_br):.3f}\")\n",
    "\n",
    "# Learn a threshold function\n",
    "#Y_train_pred_proba = classifier_best_br.predict_proba(X_sepPCs_train)\n",
    "#Y_train_pred_proba_array = Y_train_pred_proba.toarray()\n",
    "#Y_test_pred_proba = classifier_best_br.predict_proba(X_sepPCs_test)\n",
    "#Y_test_pred_proba_array = Y_test_pred_proba.toarray()\n",
    "\n",
    "t_range = (0, 1)\n",
    "\n",
    "#test_labels_binary, threshold_function = predict_test_labels_binary(Y_train_pred_proba_array, Y_train, Y_test_pred_proba_array, t_range)\n",
    "print (f\"Best parameters with threshold function learning: Hamming loss Test set is {metrics.hamming_loss(Y_test, test_labels_binary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kNN Based/learned_threshold_function.joblib']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## (CAUTION: DO NOT OVERWRITE EXISTING FILES) -- Save the kNN based best model predictions with & without a learned threshold\n",
    "##                                               and save the learned threshold\n",
    "outfile = \"kNN_bestModel_preds.npz\"\n",
    "#np.savez_compressed(outfile, Y_train_pred_best_array_br = Y_train_pred_best_array_br,\n",
    "#                             Y_test_pred_best_array_br = Y_test_pred_best_array_br,\n",
    "#                             Y_train_pred_proba_array = Y_train_pred_proba_array,\n",
    "#                             Y_test_pred_proba_array = Y_test_pred_proba_array,\n",
    "#                             test_labels_binary = test_labels_binary)\n",
    "\n",
    "outfile = \"learned_threshold_function.joblib\"\n",
    "#dump(threshold_function, outfile, compress = 3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Zhang, ML., Li, YK., Liu, XY. et al. Binary relevance for multi-label learning: an overview. Front. Comput. Sci. 12, 191–202 (2018). https://doi.org/10.1007/s11704-017-7031-7\n",
    "\n",
    "Luaces, O., Díez, J., Barranquero, J. et al. Binary relevance efficacy for multilabel classification. Prog Artif Intell 1, 303–313 (2012). https://doi.org/10.1007/s13748-012-0030-x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
