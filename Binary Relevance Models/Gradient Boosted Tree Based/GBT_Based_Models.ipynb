{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Filename: Gradient_Boosted_Trees_Based_Models.ipynb\n",
    "#\n",
    "# Purpose: Multi-label Text-categorization via binary relevance, using gradient boosted trees as base classifiers\n",
    "#\n",
    "# Author(s): Bobby (Robert) Lumpkin\n",
    "#\n",
    "# Library Dependencies: numpy, pandas, scikit-learn\n",
    "########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from joblib import dump, load\n",
    "import os\n",
    "os.chdir('C:\\\\Users\\\\rober\\\\OneDrive\\\\Documents\\\\Multilabel-Text-Classification\\\\Binary Relevance Models\\\\Gradient Boosted Tree Based')  \n",
    "## Replace above path with appropriate working directory\n",
    "import sys\n",
    "sys.path.append('../../ThresholdFunctionLearning')    ## Append path to the ThresholdFunctionLearning directory to the interpreters\n",
    "                                                   ## search path\n",
    "from threshold_learning import predict_test_labels_binary, predict_labels_binary    ## Import the 'predict_test_labels_binary()' function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set config values\n",
    "path_to_sepPCs_data = '../../Data/tfidf_PC_separable.npz'\n",
    "path_to_encoded_data = '../../Data/tfidf_encoded_data.npz'\n",
    "path_to_sepPC_gridSearch_object = 'Cache/sepPCs_gridSearch_object.joblib'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel Classification Using Binary Relevance Models\n",
    "\n",
    "Arguably, the most intuitive among multilabel modeling approaches is what's referred to as \"binary relevance\". This approach works by decomposing the multi-label learning task into a number of independent binary learning tasks (one per class label) (Zhang et al. [2018]). Binary Relevance methods are often criticized in the literature because of their label independence assumption, producing a potential weakness of ignoring correlations among labels (Luaces et al. [2012]). In this notebook, we'll explore binary relevance models built using differenct base classifiers. Later, in other notebooks, we'll train more novel approaches for comparison.\n",
    "\n",
    "For other base classifiers, one of which we consider in another notebook (kNN), we can consider different threshold function methods: constant vs. using a learned threshold function. Learning threshold functions has the advantage of allowing for different instances to possess different thresholds. This can be useful when a model either cannot consistenty separate true from false labels around a constant value OR when sufficient training is resource intensive. In many instances, a model may learn to separate true from false labels earlier in the training process than it learns to separate about a constant threshold. We do not apply these methods here, since we do not generate probability estimates.\n",
    "\n",
    "Additionally, each of the models will be trained using both the separable principal component scores and the autoencoder encodings derived in 'Preprocessing and Dimension Reduction/tfidf_Dimension_Reduction.ipynb'. Below, we'll load the data and compute one baseline for validating our models according to Hamming Loss. Namely, since our labels are sparse, we'll compute the validation Hamming Loss associated with a constant zero classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the 'separable' PC features\n",
    "npzfile = np.load(path_to_sepPCs_data)\n",
    "X_sepPCs_train = npzfile[\"X_sepPCs_train\"]\n",
    "X_sepPCs_test = npzfile[\"X_sepPCs_test\"]\n",
    "\n",
    "## Load the autoencoder encodings\n",
    "npzfile = np.load(path_to_encoded_data)\n",
    "encoded_train = npzfile[\"encoded_train\"]\n",
    "encoded_test = npzfile[\"encoded_test\"]\n",
    "\n",
    "## Load the labels\n",
    "Y_train = npzfile[\"Y_train\"]\n",
    "Y_test = npzfile[\"Y_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013779397151374627"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Compute the validation Hamming Loss for a constant zero classifier (used as silly baseline for sparse labels)\n",
    "prop_one_bpmll = np.sum(Y_test == 1) / (Y_test.shape[0] * Y_test.shape[1])\n",
    "prop_one_bpmll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Classifier: Gradient Boosted Decision Trees\n",
    "## PCA Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Hamming loss for the training data is 0.001\n",
      "The Hamming loss for the test data is 0.006\n"
     ]
    }
   ],
   "source": [
    "## Implement a binary relevance model using GBT classifiers (Naive approach to be compared with novel approaches, later)\n",
    "br_classifier = BinaryRelevance(\n",
    "    classifier = GradientBoostingClassifier(learning_rate = 0.1,\n",
    "                                            n_estimators = 100,\n",
    "                                            max_depth = 3,\n",
    "                                            random_state = 123)\n",
    ")\n",
    "\n",
    "#br_classifier.fit(X_sepPCs_train, Y_train)\n",
    "\n",
    "#br_train_preds = br_classifier.predict(X_sepPCs_train).toarray() ## -- Making predictions takes some time. \n",
    "#br_test_preds = br_classifier.predict(X_sepPCs_test).toarray()      ## Instead, load the predictions from 'SVM_based_preds.npz', on next line.\n",
    "\n",
    "npzfile = npzfile = np.load(\"GBT_based_preds.npz\", allow_pickle = True)\n",
    "br_train_preds = npzfile[\"br_train_preds\"]\n",
    "br_test_preds = npzfile[\"br_test_preds\"]\n",
    "\n",
    "print (f\"The Hamming loss for the training data is {metrics.hamming_loss(Y_train, br_train_preds):.3f}\")\n",
    "print (f\"The Hamming loss for the test data is {metrics.hamming_loss(Y_test, br_test_preds):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (CAUTION: DO NOT OVERWRITE EXISTING FILES) -- Save the GBT based predictions\n",
    "outfile = \"GBT_based_preds\"\n",
    "#np.savez_compressed(outfile, br_train_preds = br_train_preds,\n",
    "#                             br_test_preds = br_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "random.seed(123)\n",
    "n_estimators_list = list(range(100, 500, 100))\n",
    "max_depth_list = list(range(1, 4))\n",
    "learning_rate_list = [0.1, 0.01, 0.001]\n",
    "parameters_br = {'classifier' : [GradientBoostingClassifier()],\n",
    "                 'classifier__n_estimators': n_estimators_list,\n",
    "                 'classifier__max_depth' : max_depth_list,\n",
    "                 'classifier__learning_rate' : learning_rate_list}  \n",
    "# By default, the Hamming loss as an option is not provided in the scoring string options. \n",
    "# So, we first define a Hamming loss scorer and use that. \n",
    "hamming_scorer = metrics.make_scorer(metrics.hamming_loss)\n",
    "\n",
    "clf_br = GridSearchCV(BinaryRelevance(), parameters_br, scoring = hamming_scorer, cv = 5, verbose = 1)\n",
    "clf_br.fit(X_sepPCs_train, Y_train) #-- To save time, load the pre-fit grid search object in the next line.\n",
    "#clf_br = load(\"clf_br_gridSearch_object.joblib\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (CAUTION: DO NOT OVERWRITE EXISTING FILES) -- Save the kNN based grid search object\n",
    "dump(clf_br, path_to_sepPC_gridSearch_object, compress = 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'cv_results_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-21d0e14aba8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbest_index_br\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf_br\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"mean_test_score\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mbest_parameters_br\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf_br\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"params\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbest_index_br\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_CV_br\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Params\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Mean out-of-bag Hamming loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf_CV_br\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Params\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf_br\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"params\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'cv_results_'"
     ]
    }
   ],
   "source": [
    "best_index_br = np.argmin(clf_br.cv_results_[\"mean_test_score\"])\n",
    "best_parameters_br = clf_br.cv_results_[\"params\"][best_index_br]\n",
    "\n",
    "df_CV_br = pd.DataFrame(columns=[\"Params\", \"Mean out-of-bag Hamming loss\"])\n",
    "df_CV_br[\"Params\"] = clf_br.cv_results_[\"params\"]\n",
    "df_CV_br[ \"Mean out-of-bag Hamming loss\"] = clf_br.cv_results_[\"mean_test_score\"]\n",
    "display(df_CV_br)\n",
    "print(f\"Best parameters: {best_parameters_br}. Best mean out-of-bag Hamming loss: {np.min(clf_br.cv_results_['mean_test_score'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
