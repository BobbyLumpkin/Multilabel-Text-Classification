{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Filename: Binary_Relevance_Classification.ipynb\n",
    "#\n",
    "# Purpose: Multi-label Text-categorization via binary relevance, using k-nearest neighbors, gradient boosted trees, \n",
    "#          and support vector machines.\n",
    "\n",
    "# Author(s): Bobby (Robert) Lumpkin, Archit Datar (kNN)\n",
    "#\n",
    "# Library Dependencies: numpy, pandas, scikit-learn\n",
    "########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.neighbors import KNeighborsClassifier as kNN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from joblib import dump, load\n",
    "import sys\n",
    "sys.path.append('../ThresholdFunctionLearning')    ## Append path to the ThresholdFunctionLearning directory to the interpreters\n",
    "                                                   ## search path\n",
    "from threshold_learning import predict_test_labels_binary    ## Import the 'predict_test_labels_binary()' function \n",
    "import os\n",
    "os.chdir('C:\\\\Users\\\\rober\\\\OneDrive\\\\Documents\\\\Multilabel-Text-Classification\\\\Binary Relevance Models')  \n",
    "## Replace with above path with appropriate working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel Classification Using Binary Relevance Models\n",
    "\n",
    "Arguably, the most intuitive among multilabel modeling approaches is what's referred to as \"binary relevance\". This approach works by decomposing the multi-label learning task into a number of independent binary learning tasks (one per class label) (Zhang et al. [2018]). Binary Relevance methods are often criticized in the literature because of their label independence assumption, producing a potential weakness of ignoring correlations among labels (Luaces et al. [2012]). In this notebook, we'll explore binary relevance models built using differenct base classifiers. Later, in other notebooks, we'll train more novel approaches for comparison.\n",
    "\n",
    "Each of the models will be trained using both the separable principal component scores and the autoencoder encodings derived in 'Preprocessing and Dimension Reduction/tfidf_Dimension_Reduction.ipynb'. Below, we'll load the data and compute one baseline for validating our models according to Hamming Loss. Namely, since our labels are sparse, we'll compute the validation Hamming Loss associated with a constant zero classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the 'separable' PC features\n",
    "npzfile = np.load(\"../Data/tfidf_PC_separable.npz\")\n",
    "X_sepPCs_train = npzfile[\"X_sepPCs_train\"]\n",
    "X_sepPCs_test = npzfile[\"X_sepPCs_test\"]\n",
    "\n",
    "## Load the autoencoder encodings\n",
    "npzfile = np.load(\"../Data/tfidf_encoded_data.npz\")\n",
    "encoded_train = npzfile[\"encoded_train\"]\n",
    "encoded_test = npzfile[\"encoded_test\"]\n",
    "\n",
    "## Load the labels\n",
    "Y_train = npzfile[\"Y_train\"]\n",
    "Y_test = npzfile[\"Y_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013779397151374627"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Compute the validation Hamming Loss for a constant zero classifier (used as silly baseline for sparse labels)\n",
    "prop_one_bpmll = np.sum(Y_test == 1) / (Y_test.shape[0] * Y_test.shape[1])\n",
    "prop_one_bpmll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Classifier: k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Hamming loss for the training data is 0.003\n",
      "The Hamming loss for the test data is 0.005\n"
     ]
    }
   ],
   "source": [
    "## Implement a binary relevance model using KNN classifiers (Naive approach to be compared with ML-KNN, later)\n",
    "br_classifier = BinaryRelevance(\n",
    "    classifier = kNN()\n",
    ")\n",
    "\n",
    "br_classifier.fit(X_sepPCs_train, Y_train)\n",
    "\n",
    "#br_train_preds = br_classifier.predict(X_sepPCs_train).toarray() -- Making predictions takes some time. \n",
    "#br_test_preds = br_classifier.predict(X_sepPCs_test).toarray()      Instead, load the predictions from 'kNN_based_preds.npz', on next line.\n",
    "\n",
    "npzfile = npzfile = np.load(\"kNN Based/kNN_based_preds.npz\", allow_pickle = True)\n",
    "br_train_preds = npzfile[\"br_train_preds\"]\n",
    "br_test_preds = npzfile[\"br_test_preds\"]\n",
    "\n",
    "print (f\"The Hamming loss for the training data is {metrics.hamming_loss(Y_train, br_train_preds):.3f}\")\n",
    "print (f\"The Hamming loss for the test data is {metrics.hamming_loss(Y_test, br_test_preds):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (CAUTION: DO NOT OVERWRITE EXISTING FILES) -- Save the kNN based predictions\n",
    "outfile = \"kNN Based/kNN_based_preds\"\n",
    "#np.savez_compressed(outfile, br_train_preds = br_train_preds,\n",
    "#                             br_test_preds = br_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "n_neighbors_list = list(range(3, 16, 3))\n",
    "n_neighbors_list.insert(0, 1)\n",
    "parameters_br = {'n_neighbors': n_neighbors_list}  \n",
    "# By default, the Hamming loss as an option is not provided in the scoring string options. \n",
    "# So, we first define a Hamming loss scorer and use that. \n",
    "hamming_scorer = metrics.make_scorer(metrics.hamming_loss)\n",
    "\n",
    "clf_br = GridSearchCV(kNN(), parameters_br, scoring = hamming_scorer, cv = 5, verbose = 1)\n",
    "#clf_br.fit(X_sepPCs_train, Y_train) #-- To save time, load the pre-fit grid search object in the next line.\n",
    "clf_br = load(\"kNN Based/clf_br_gridSearch_object\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kNN Based/clf_br_gridSearch_object']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## (CAUTION: DO NOT OVERWRITE EXISTING FILES) -- Save the kNN based predictions\n",
    "outfile = \"kNN Based/clf_br_gridSearch_object\"\n",
    "#dump(clf_br, outfile, compress = 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Params</th>\n",
       "      <th>Mean out-of-bag Hamming loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'n_neighbors': 1}</td>\n",
       "      <td>0.005997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'n_neighbors': 3}</td>\n",
       "      <td>0.004788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'n_neighbors': 6}</td>\n",
       "      <td>0.004697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'n_neighbors': 9}</td>\n",
       "      <td>0.004714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'n_neighbors': 12}</td>\n",
       "      <td>0.004881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'n_neighbors': 15}</td>\n",
       "      <td>0.004877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Params  Mean out-of-bag Hamming loss\n",
       "0   {'n_neighbors': 1}                      0.005997\n",
       "1   {'n_neighbors': 3}                      0.004788\n",
       "2   {'n_neighbors': 6}                      0.004697\n",
       "3   {'n_neighbors': 9}                      0.004714\n",
       "4  {'n_neighbors': 12}                      0.004881\n",
       "5  {'n_neighbors': 15}                      0.004877"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_neighbors': 6}, Best mean out-of-bag Hamming loss: 0.004696695591737446\n"
     ]
    }
   ],
   "source": [
    "best_index_br = np.argmin(clf_br.cv_results_[\"mean_test_score\"])\n",
    "best_parameters_br = clf_br.cv_results_[\"params\"][best_index_br]\n",
    "\n",
    "df_CV_br = pd.DataFrame(columns=[\"Params\", \"Mean out-of-bag Hamming loss\"])\n",
    "df_CV_br[\"Params\"] = clf_br.cv_results_[\"params\"]\n",
    "df_CV_br[ \"Mean out-of-bag Hamming loss\"] = clf_br.cv_results_[\"mean_test_score\"]\n",
    "display(df_CV_br)\n",
    "print(f\"Best parameters: {best_parameters_br}, Best mean out-of-bag Hamming loss: {np.min(clf_br.cv_results_['mean_test_score'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Zhang, ML., Li, YK., Liu, XY. et al. Binary relevance for multi-label learning: an overview. Front. Comput. Sci. 12, 191–202 (2018). https://doi.org/10.1007/s11704-017-7031-7\n",
    "\n",
    "Luaces, O., Díez, J., Barranquero, J. et al. Binary relevance efficacy for multilabel classification. Prog Artif Intell 1, 303–313 (2012). https://doi.org/10.1007/s13748-012-0030-x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
