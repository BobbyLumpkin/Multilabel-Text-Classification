{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Filename: Text_Preprocessing.ipynb\n",
    "#\n",
    "# Purpose: Preprocess text data using both bag-of-words and sequence of integers approaches.\n",
    "#\n",
    "# Author(s): Bobby (Robert) Lumpkin\n",
    "#\n",
    "# Library Dependencies: numpy, pandas, tensorflow, bpmll, scipy, random, os, re, string, json, collections, \n",
    "#                       sklearn, nltk, \n",
    "#\n",
    "# NOTES: (1) A lot of the data cleaning performed here is due to a great tutorial written by Patrick Loeber\n",
    "#        which can be found at: https://github.com/python-engineer/tensorflow-course/blob/master/11_NLP.ipynb\n",
    "#        (2) The code organizing the Reuters-21578 dataset into a pandas dataframe came from Kaggle\n",
    "#        and can be found at: https://www.kaggle.com/boldy717/reutersnltk\n",
    "########################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing for Multilabel Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from bpmll import bp_mll_loss\n",
    "import sklearn_json as skljson\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import reuters    ## This downloads the reduced Reuters-21578 dataset\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Reuters-21578 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FRENCH',\n",
       " 'FREE',\n",
       " 'MARKET',\n",
       " 'CEREAL',\n",
       " 'EXPORT',\n",
       " 'BIDS',\n",
       " 'DETAILED',\n",
       " 'French',\n",
       " 'operators',\n",
       " 'have',\n",
       " 'requested',\n",
       " 'licences',\n",
       " 'to',\n",
       " 'export']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.words('training/9865')[:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['barley', 'corn', 'grain', 'wheat']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.categories('training/9865')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract fileids from the reuters corpus\n",
    "fileids = reuters.fileids()\n",
    "\n",
    "# Initialize empty lists to store categories and raw text\n",
    "categories = []\n",
    "text = []\n",
    "\n",
    "# Loop through each file id and collect each files categories and raw text\n",
    "for file in fileids:\n",
    "    categories.append(reuters.categories(file))\n",
    "    text.append(reuters.raw(file))\n",
    "\n",
    "# Combine lists into pandas dataframe. reutersDf is the final dataframe. \n",
    "reutersDF = pd.DataFrame({'ids':fileids, 'categories':categories, 'text':text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>categories</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test/14826</td>\n",
       "      <td>[trade]</td>\n",
       "      <td>ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test/14828</td>\n",
       "      <td>[grain]</td>\n",
       "      <td>CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test/14829</td>\n",
       "      <td>[crude, nat-gas]</td>\n",
       "      <td>JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test/14832</td>\n",
       "      <td>[corn, grain, rice, rubber, sugar, tin, trade]</td>\n",
       "      <td>THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test/14833</td>\n",
       "      <td>[palm-oil, veg-oil]</td>\n",
       "      <td>INDONESIA SEES CPO PRICE RISING SHARPLY\\n  Ind...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ids                                      categories  \\\n",
       "0  test/14826                                         [trade]   \n",
       "1  test/14828                                         [grain]   \n",
       "2  test/14829                                [crude, nat-gas]   \n",
       "3  test/14832  [corn, grain, rice, rubber, sugar, tin, trade]   \n",
       "4  test/14833                             [palm-oil, veg-oil]   \n",
       "\n",
       "                                                text  \n",
       "0  ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...  \n",
       "1  CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...  \n",
       "2  JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...  \n",
       "3  THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...  \n",
       "4  INDONESIA SEES CPO PRICE RISING SHARPLY\\n  Ind...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reutersDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check if there are any urls in articles\n",
    "pattern = re.compile(r\"https?://(\\S+|www)\\.\\S+\")\n",
    "for t in reutersDF.text:\n",
    "    matches = pattern.findall(t)\n",
    "    for match in matches:\n",
    "        print(t)\n",
    "        print(match)\n",
    "        print(pattern.sub(r\"\", t))\n",
    "    if len(matches) > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Define a function to remove punctuation from documents\n",
    "def remove_punct(text):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove punctuation from documents\n",
    "reutersDF[\"text\"] = reutersDF.text.apply(remove_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rober\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Define a function to remove stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Stop Words: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine\n",
    "# has been programmed to ignore, both when indexing entries for searching and when retrieving them \n",
    "# as the result of a search query.\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove stopwords\n",
    "reutersDF[\"text\"] = reutersDF.text.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>categories</th>\n",
       "      <th>text</th>\n",
       "      <th>acq</th>\n",
       "      <th>alum</th>\n",
       "      <th>barley</th>\n",
       "      <th>bop</th>\n",
       "      <th>carcass</th>\n",
       "      <th>castor-oil</th>\n",
       "      <th>cocoa</th>\n",
       "      <th>...</th>\n",
       "      <th>sun-oil</th>\n",
       "      <th>sunseed</th>\n",
       "      <th>tea</th>\n",
       "      <th>tin</th>\n",
       "      <th>trade</th>\n",
       "      <th>veg-oil</th>\n",
       "      <th>wheat</th>\n",
       "      <th>wpi</th>\n",
       "      <th>yen</th>\n",
       "      <th>zinc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test/14826</td>\n",
       "      <td>[trade]</td>\n",
       "      <td>asian exporters fear damage usjapan rift mount...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test/14828</td>\n",
       "      <td>[grain]</td>\n",
       "      <td>china daily says vermin eat 712 pct grain stoc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test/14829</td>\n",
       "      <td>[crude, nat-gas]</td>\n",
       "      <td>japan revise longterm energy demand downwards ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test/14832</td>\n",
       "      <td>[corn, grain, rice, rubber, sugar, tin, trade]</td>\n",
       "      <td>thai trade deficit widens first quarter thaila...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test/14833</td>\n",
       "      <td>[palm-oil, veg-oil]</td>\n",
       "      <td>indonesia sees cpo price rising sharply indone...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 93 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ids                                      categories  \\\n",
       "0  test/14826                                         [trade]   \n",
       "1  test/14828                                         [grain]   \n",
       "2  test/14829                                [crude, nat-gas]   \n",
       "3  test/14832  [corn, grain, rice, rubber, sugar, tin, trade]   \n",
       "4  test/14833                             [palm-oil, veg-oil]   \n",
       "\n",
       "                                                text  acq  alum  barley  bop  \\\n",
       "0  asian exporters fear damage usjapan rift mount...    0     0       0    0   \n",
       "1  china daily says vermin eat 712 pct grain stoc...    0     0       0    0   \n",
       "2  japan revise longterm energy demand downwards ...    0     0       0    0   \n",
       "3  thai trade deficit widens first quarter thaila...    0     0       0    0   \n",
       "4  indonesia sees cpo price rising sharply indone...    0     0       0    0   \n",
       "\n",
       "   carcass  castor-oil  cocoa  ...  sun-oil  sunseed  tea  tin  trade  \\\n",
       "0        0           0      0  ...        0        0    0    0      1   \n",
       "1        0           0      0  ...        0        0    0    0      0   \n",
       "2        0           0      0  ...        0        0    0    0      0   \n",
       "3        0           0      0  ...        0        0    0    1      1   \n",
       "4        0           0      0  ...        0        0    0    0      0   \n",
       "\n",
       "   veg-oil  wheat  wpi  yen  zinc  \n",
       "0        0      0    0    0     0  \n",
       "1        0      0    0    0     0  \n",
       "2        0      0    0    0     0  \n",
       "3        0      0    0    0     0  \n",
       "4        1      0    0    0     0  \n",
       "\n",
       "[5 rows x 93 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Generate binary labels from 'categories'\n",
    "mlb = MultiLabelBinarizer()\n",
    "binary_labels = mlb.fit_transform(reutersDF.categories)\n",
    "labelsDF = pd.DataFrame(binary_labels, columns = mlb.classes_)\n",
    "\n",
    "reutersDF = pd.concat([reutersDF, labelsDF], axis = 1)\n",
    "reutersDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (CAUTION: DO NOT OVERWRITE EXISTING FILES) -- Save the label list\n",
    "label_list = list(reutersDF.columns[3:])\n",
    "label_list_dict = {'label_list' : label_list}\n",
    "#with open(\"Data/label_list.json\", \"w\") as outfile: \n",
    "#    json.dump(label_list, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating TF-IDF Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate tf-idf vectors for each instance\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(reutersDF.text.tolist())\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "tf_idf_DF = pd.DataFrame(dense, columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the training/test data \n",
    "label_cols = reutersDF.columns[3:]\n",
    "X_tfidf = dense\n",
    "Y_tfidf = reutersDF[label_cols].to_numpy()\n",
    "\n",
    "training_indices = [index for index in reutersDF.index if 'training' in reutersDF.loc[index,].ids]\n",
    "test_indices = [index for index in reutersDF.index if 'test' in reutersDF.loc[index,].ids]\n",
    "\n",
    "X_tfidfTrain = X_tfidf[training_indices,]\n",
    "X_tfidfTest = X_tfidf[test_indices]\n",
    "Y_tfidfTrain = Y_tfidf[training_indices]\n",
    "Y_tfidfTest = Y_tfidf[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (CAUTION: DO NOT OVERWRITE EXISTING FILES) -- Write the training and test data to a .npz file\n",
    "outfile = \"Data/tfidf_trainTest_data.npz\"\n",
    "#np.savez_compressed(outfile, X_tfidfTrain = X_tfidfTrain, \n",
    "#                             X_tfidfTest = X_tfidfTest, \n",
    "#                             Y_tfidfTrain = Y_tfidfTrain, \n",
    "#                             Y_tfidfTest = Y_tfidfTest,\n",
    "#                             feature_names = feature_names)\n",
    "#feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Sequence of Integers Feature Vectors (for RNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('said', 25376), ('mln', 18598), ('vs', 14340), ('dlrs', 12326), ('pct', 9771)]\n",
      "51028\n"
     ]
    }
   ],
   "source": [
    "# Count unique words\n",
    "def counter_word(text_col):\n",
    "    count = Counter()\n",
    "    for text in text_col.values:\n",
    "        for word in text.split():\n",
    "            count[word] += 1\n",
    "    return count\n",
    "\n",
    "counter = counter_word(reutersDF.text)\n",
    "num_unique_words = len(counter)\n",
    "print(counter.most_common(5))\n",
    "print(num_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert text column to numpy array and train/test split\n",
    "X_seq = reutersDF.text.to_numpy()\n",
    "X_seqTrain = X_seq[training_indices, ]\n",
    "X_seqTest = X_seq[test_indices, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bahia cocoa review showers continued throughout week bahia cocoa zone alleviating drought since early january improving prospects coming temporao although normal humidity levels restored comissaria smith said weekly review dry period means temporao late year arrivals week ended february 22 155221 bags 60 kilos making cumulative total season 593 mln 581 stage last year seems cocoa delivered earlier consignment included arrivals figures comissaria smith said still doubt much old crop cocoa still available harvesting practically come end total bahia crop estimates around 64 mln bags sales standing almost 62 mln hundred thousand bags still hands farmers middlemen exporters processors doubts much cocoa would fit export shippers experiencing dificulties obtaining bahia superior certificates view lower quality recent weeks farmers sold good part cocoa held consignment comissaria smith said spot bean prices rose 340 350 cruzados per arroba 15 kilos bean shippers reluctant offer nearby shipment limited sales booked march shipment 1750 1780 dlrs per tonne ports named new crop sales also light open ports junejuly going 1850 1880 dlrs 35 45 dlrs new york july augsept 1870 1875 1880 dlrs per tonne fob routine sales butter made marchapril sold 4340 4345 4350 dlrs aprilmay butter went 227 times new york may junejuly 4400 4415 dlrs augsept 4351 4450 dlrs 227 228 times new york sept octdec 4480 dlrs 227 times new york dec comissaria smith said destinations us covertible currency areas uruguay open ports cake sales registered 785 995 dlrs marchapril 785 dlrs may 753 dlrs aug 039 times new york dec octdec buyers us argentina uruguay convertible currency areas liquor sales limited marchapril selling 2325 2380 dlrs junejuly 2375 dlrs 125 times new york july augsept 2400 dlrs 125 times new york sept octdec 125 times new york dec comissaria smith said total bahia sales currently estimated 613 mln bags 198687 crop 106 mln bags 198788 crop final figures period february 28 expected published brazilian cocoa trade commission carnival ends midday february 27\n",
      "[5585, 398, 739, 5586, 582, 1959, 59, 5585, 398, 1901, 20908, 1390, 158, 259, 39, 2212, 873, 1217, 3799, 499, 926, 8081, 308, 4086, 9026, 1636, 1, 1044, 739, 1653, 180, 914, 3799, 416, 7, 5278, 59, 188, 38, 306, 20909, 955, 545, 2295, 447, 1352, 76, 627, 4596, 2, 7377, 1611, 18, 7, 2249, 398, 2590, 92, 10197, 639, 5278, 198, 9026, 1636, 1, 226, 2894, 305, 1673, 353, 398, 226, 513, 3438, 10198, 588, 131, 76, 5585, 353, 618, 146, 1141, 2, 955, 35, 5927, 922, 1587, 2, 4795, 4796, 955, 226, 1837, 454, 14957, 611, 2537, 4237, 305, 398, 12, 3930, 94, 7378, 9027, 20910, 3800, 5585, 4797, 758, 1078, 136, 1438, 246, 317, 454, 251, 445, 272, 398, 326, 10197, 9026, 1636, 1, 1187, 4238, 34, 57, 3146, 1374, 6341, 37, 20911, 78, 2295, 4238, 7378, 3658, 49, 2213, 796, 776, 35, 3554, 32, 796, 2712, 12041, 4, 37, 543, 1449, 1353, 25, 353, 35, 28, 1045, 429, 1449, 10199, 784, 5928, 10200, 4, 441, 544, 4, 25, 334, 552, 9028, 14958, 9029, 10200, 4, 37, 543, 2031, 4597, 35, 4598, 122, 6806, 251, 20912, 14959, 12042, 4, 5279, 4598, 1736, 3339, 970, 25, 334, 43, 10199, 9030, 14960, 4, 9028, 20913, 14961, 4, 3339, 2649, 970, 25, 334, 3931, 6342, 20914, 4, 3339, 970, 25, 334, 729, 9026, 1636, 1, 2214, 10, 20915, 166, 525, 2296, 429, 1449, 6343, 35, 1510, 9031, 9032, 4, 6806, 9031, 4, 43, 5587, 4, 6807, 14962, 970, 25, 334, 729, 6342, 1354, 10, 2372, 2296, 1031, 166, 525, 10201, 35, 776, 6806, 559, 20916, 20917, 4, 10199, 20918, 4, 792, 970, 25, 334, 552, 9028, 5588, 4, 792, 970, 25, 334, 3931, 6342, 792, 970, 25, 334, 729, 9026, 1636, 1, 76, 5585, 35, 479, 239, 12043, 2, 955, 414, 353, 1755, 2, 955, 777, 353, 424, 198, 180, 38, 213, 64, 1902, 730, 398, 19, 189, 8082, 1142, 3439, 38, 371]\n"
     ]
    }
   ],
   "source": [
    "## Tokenize and vectorize, turning each text into a sequence of integers\n",
    "random.seed(123)\n",
    "\n",
    "tokenizer = Tokenizer(num_words = num_unique_words)\n",
    "tokenizer.fit_on_texts(X_seqTrain)    # Fit only to training\n",
    "word_index = tokenizer.word_index\n",
    "train_sequences = tokenizer.texts_to_sequences(X_seqTrain)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_seqTest)\n",
    "\n",
    "print(X_seqTrain[0])\n",
    "print(train_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7769, 827), (3019, 827))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pad the sequences to have the same length (Max number of words in a sequence)\n",
    "max_doc_length = 0\n",
    "for doc in train_sequences:\n",
    "    if len(doc) > max_doc_length:\n",
    "        max_doc_length = len(doc)\n",
    "        \n",
    "max_length = max_doc_length\n",
    "\n",
    "train_padded = pad_sequences(train_sequences, maxlen = max_length, padding = \"post\", truncating = \"post\")\n",
    "test_padded = pad_sequences(test_sequences, maxlen = max_length, padding = \"post\", truncating = \"post\")\n",
    "train_padded.shape, test_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (CAUTION: DO NOT OVERWRITE EXISTING FILES) -- Write the training and test data to a .npz file\n",
    "outfile = \"Data/seq_trainTest_data.npz\"\n",
    "#np.savez_compressed(outfile, train_padded = train_padded, \n",
    "#                             test_padded = test_padded, \n",
    "#                             Y_train = Y_tfidfTrain, \n",
    "#                             Y_test = Y_tfidfTest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
